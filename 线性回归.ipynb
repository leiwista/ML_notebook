{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n",
    "线性回归问题是最基本的一种ML方法，其中使用线性方程作为预测方程h(x)。解决线性回归问题的过程其实就是找到线性方程中合适的参数的过程。\n",
    "基本步骤可以分为：\n",
    "\n",
    "1. 使用有限的Train Set找到最匹配的线性预测方程(hypothesis function)参数；\n",
    "2. 使用得到的预测方程预测实际问题；\n",
    "\n",
    "其中预测方程一般定义为\n",
    "### hypothesis函数\n",
    "$$ h(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdot \\cdot \\cdot + \\theta_n x_n $$ \n",
    "n个feature的线性回归问题拥有n+1个参数，如果使用矩阵表示\\\\( h(X) = \\Theta ^ T X \\\\)\n",
    "其中的\\\\( \\Theta \\\\)和\\\\( X \\\\)都是一个列向量，\\\\( \\begin{bmatrix}\n",
    "\\theta_0 \\\\ \n",
    "\\theta_1 \\\\ \n",
    "\\theta_2 \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "\\\\) 和 \\\\( \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\ \n",
    "x_2 \\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "\\\\)。为了使用Train Set得到最拟合的参数\\\\( \\Theta \\\\)，提出Cost函数，考量所有Train Set中预测值和真实值之间的差异和，使得该和最小化。\n",
    "为了找到使该Cost函数最小的参数\\\\( \\Theta \\\\)，使用了比较常见的梯度下降算法。\n",
    "\n",
    "### 代价函数\n",
    "$$ Cost(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x^i) - y^i)^2 $$\n",
    "\n",
    "### 梯度下降算法\n",
    "repeat until convergence {\n",
    "\n",
    "\\\\( \\theta_i := \\theta_i - \\alpha \\frac{\\partial Cost(\\Theta)}{\\partial \\theta_i} \\\\)\n",
    "\n",
    "}\n",
    "\n",
    "其中\\\\( \\frac{\\partial Cost(\\Theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^i) - y^i) x_{j}^{i} \\\\)，而\\\\( \\alpha \\\\)决定了单次迭代的步长，如果太小，可能要经过很多次才能得到绝对最优解，而太大的话，可能永远无法收敛到最优解，一直在来回震荡。\n",
    "\n",
    "如果按照矩阵方式进行该算法计算，其中关键的偏导可以表示为\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{1}^{1}  & x_{1}^{2} & .  & . & . & x_{1}^{m} \\\\\n",
    "x_{2}^{1}  & x_{2}^{2} & .  & . & . & x_{2}^{m} \\\\\n",
    "x_{3}^{1}  & x_{3}^{2} & .  & . & . & x_{3}^{m} \\\\\n",
    ". & . & .  & . & . & . \\\\\n",
    "x_{n}^{1}  & x_{n}^{2} & .  & . & . & x_{n}^{m} \\\\\n",
    "\\end{bmatrix}\n",
    ".\n",
    "\\begin{bmatrix}\n",
    "h(X^{1}) - y^1)\\\\ \n",
    "h(X^{2}) - y^2)\\\\ \n",
    "h(X^{3}) - y^3)\\\\ \n",
    ".\\\\\n",
    "h(X^{m}) - y^m)\\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \\sum_{i=1}^{m} (h(x^i) - y^i) x_{1}^{i} \\\\ \n",
    " \\sum_{i=1}^{m} (h(x^i) - y^i) x_{2}^{i} \\\\ \n",
    " \\sum_{i=1}^{m} (h(x^i) - y^i) x_{3}^{i} \\\\ \n",
    ". \\\\\n",
    " \\sum_{i=1}^{m} (h(x^i) - y^i) x_{n}^{i} \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
